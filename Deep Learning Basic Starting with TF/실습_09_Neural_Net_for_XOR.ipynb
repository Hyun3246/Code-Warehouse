{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "zehEsS3u_r8y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [[0, 0],\n",
        "          [0, 1],\n",
        "          [1, 0],\n",
        "          [1, 1]]\n",
        "y_data = [[0],\n",
        "          [1],\n",
        "          [1],\n",
        "          [0]]\n",
        "\n",
        "plt.scatter(x_data[0][0],x_data[0][1], c='red' , marker='^')\n",
        "plt.scatter(x_data[3][0],x_data[3][1], c='red' , marker='^')\n",
        "plt.scatter(x_data[1][0],x_data[1][1], c='blue' , marker='^')\n",
        "plt.scatter(x_data[2][0],x_data[2][1], c='blue' , marker='^')\n",
        "\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "J1ewPGYI_u-Z",
        "outputId": "eeb779d4-ee4b-406d-b977-5844c0fc3fd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show(close=None, block=None)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>matplotlib.pyplot.show</b><br/>def show(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py</a>Display all open figures.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "block : bool, optional\n",
              "    Whether to wait for all figures to be closed before returning.\n",
              "\n",
              "    If `True` block and run the GUI main loop until all figure windows\n",
              "    are closed.\n",
              "\n",
              "    If `False` ensure that all figure windows are displayed and return\n",
              "    immediately.  In this case, you are responsible for ensuring\n",
              "    that the event loop is running to have responsive figures.\n",
              "\n",
              "    Defaults to True in non-interactive mode and to False in interactive\n",
              "    mode (see `.pyplot.isinteractive`).\n",
              "\n",
              "See Also\n",
              "--------\n",
              "ion : Enable interactive mode, which shows / updates the figure after\n",
              "      every plotting command, so that calling ``show()`` is not necessary.\n",
              "ioff : Disable interactive mode.\n",
              "savefig : Save the figure to an image file instead of showing it on screen.\n",
              "\n",
              "Notes\n",
              "-----\n",
              "**Saving figures to file and showing a window at the same time**\n",
              "\n",
              "If you want an image file as well as a user interface window, use\n",
              "`.pyplot.savefig` before `.pyplot.show`. At the end of (a blocking)\n",
              "``show()`` the figure is closed and thus unregistered from pyplot. Calling\n",
              "`.pyplot.savefig` afterwards would save a new and thus empty figure. This\n",
              "limitation of command order does not apply if the show is non-blocking or\n",
              "if you keep a reference to the figure and use `.Figure.savefig`.\n",
              "\n",
              "**Auto-show in jupyter notebooks**\n",
              "\n",
              "The jupyter backends (activated via ``%matplotlib inline``,\n",
              "``%matplotlib notebook``, or ``%matplotlib widget``), call ``show()`` at\n",
              "the end of every cell by default. Thus, you usually don&#x27;t have to call it\n",
              "explicitly there.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 482);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\n",
        "\n",
        "def preprocess_data(features, labels):\n",
        "    features = tf.cast(features, tf.float32)\n",
        "    labels = tf.cast(labels, tf.float32)\n",
        "    return features, labels"
      ],
      "metadata": {
        "id": "nHwTYk0nAAQa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression으로 해결"
      ],
      "metadata": {
        "id": "qbHxkCVuCF7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = tf.Variable(tf.zeros([2,1]), name='weight')\n",
        "b = tf.Variable(tf.zeros([1]), name='bias')\n",
        "print(\"W = {}, B = {}\".format(W.numpy(), b.numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_UpG2jkAQ8t",
        "outputId": "e080eff5-37d9-4bdd-e779-cdf6604b07dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W = [[0.]\n",
            " [0.]], B = [0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sigmoid 함수 가설 함수로 설정\n",
        "def logistic_regression(features):\n",
        "    hypothesis = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b))\n",
        "    return hypothesis"
      ],
      "metadata": {
        "id": "6SuPXbK5AYlY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(hypothesis, features, labels):\n",
        "    cost = -tf.reduce_mean(labels * tf.math.log(logistic_regression(features)) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
        "    return cost\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)"
      ],
      "metadata": {
        "id": "nXQoHZV1ApHB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(hypothesis, labels):\n",
        "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "zK8ZbUVCA3-h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grad(hypothesis, features, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss_fn(logistic_regression(features),features,labels)\n",
        "    return tape.gradient(loss_value, [W,b])"
      ],
      "metadata": {
        "id": "ExYi0gy3BjFf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1001\n",
        "\n",
        "for step in range(epochs):\n",
        "    for features, labels in dataset:\n",
        "        features, labels = preprocess_data(features, labels)\n",
        "        grads = grad(logistic_regression(features), features, labels)\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))\n",
        "        if step % 100 == 0:\n",
        "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features),features,labels)))\n",
        "print(\"W = {}, B = {}\".format(W.numpy(), b.numpy()))\n",
        "x_data, y_data = preprocess_data(x_data, y_data)\n",
        "test_acc = accuracy_fn(logistic_regression(x_data),y_data)\n",
        "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4LNjeLyBL23",
        "outputId": "adbc1e95-a81f-45c8-f602-bec0350810b4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 0, Loss: 0.6931\n",
            "Iter: 100, Loss: 0.6931\n",
            "Iter: 200, Loss: 0.6931\n",
            "Iter: 300, Loss: 0.6931\n",
            "Iter: 400, Loss: 0.6931\n",
            "Iter: 500, Loss: 0.6931\n",
            "Iter: 600, Loss: 0.6931\n",
            "Iter: 700, Loss: 0.6931\n",
            "Iter: 800, Loss: 0.6931\n",
            "Iter: 900, Loss: 0.6931\n",
            "Iter: 1000, Loss: 0.6931\n",
            "W = [[0.]\n",
            " [0.]], B = [0.]\n",
            "Testset Accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 layers의 NN으로 해결"
      ],
      "metadata": {
        "id": "kOEiOlrHCL0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = tf.Variable(tf.random.normal((2, 1)), name='weight1')\n",
        "b1 = tf.Variable(tf.random.normal((1,)), name='bias1')\n",
        "\n",
        "W2 = tf.Variable(tf.random.normal((2, 1)), name='weight2')\n",
        "b2 = tf.Variable(tf.random.normal((1,)), name='bias2')\n",
        "\n",
        "W3 = tf.Variable(tf.random.normal((2, 1)), name='weight3')\n",
        "b3 = tf.Variable(tf.random.normal((1,)), name='bias3')"
      ],
      "metadata": {
        "id": "Fpc2uFVVCSYO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_net(features):\n",
        "    layer1 = tf.sigmoid(tf.matmul(features, W1) + b1)\n",
        "    layer2 = tf.sigmoid(tf.matmul(features, W2) + b2)\n",
        "    layer3 = tf.concat([layer1, layer2], -1)\n",
        "    layer3 = tf.reshape(layer3, shape = [-1, 2])\n",
        "    hypothesis = tf.sigmoid(tf.matmul(layer3, W3) + b3)\n",
        "    return hypothesis\n",
        "\n",
        "def loss_fn(hypothesis, labels):\n",
        "    cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
        "    return cost\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "def accuracy_fn(hypothesis, labels):\n",
        "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
        "    return accuracy\n",
        "\n",
        "def grad(hypothesis, features, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss_fn(neural_net(features),labels)\n",
        "    return tape.gradient(loss_value, [W1, W2, W3, b1, b2, b3])"
      ],
      "metadata": {
        "id": "NAh0NS-RCila"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5000\n",
        "\n",
        "for step in range(EPOCHS):\n",
        "    for features, labels  in dataset:\n",
        "        features, labels = preprocess_data(features, labels)\n",
        "        grads = grad(neural_net(features), features, labels)\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W1, W2, W3, b1, b2, b3]))\n",
        "        if step % 5000 == 0:\n",
        "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(neural_net(features),labels)))\n",
        "x_data, y_data = preprocess_data(x_data, y_data)\n",
        "test_acc = accuracy_fn(neural_net(x_data),y_data)\n",
        "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqnRQM_uDBsB",
        "outputId": "759fc7cb-52a0-496a-b75e-85e878a828cb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 0, Loss: 0.5915\n",
            "Testset Accuracy: 0.7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Neural Network로 해결"
      ],
      "metadata": {
        "id": "05s4gbKwEnj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\n",
        "nb_classes = 10\n",
        "\n",
        "class wide_deep_nn():\n",
        "    def __init__(self, nb_classes):\n",
        "        super(wide_deep_nn, self).__init__()\n",
        "\n",
        "        self.W1 = tf.Variable(tf.random.normal((2, nb_classes)), name='weight1')\n",
        "        self.b1 = tf.Variable(tf.random.normal((nb_classes,)), name='bias1')\n",
        "\n",
        "        self.W2 = tf.Variable(tf.random.normal((nb_classes, nb_classes)), name='weight2')\n",
        "        self.b2 = tf.Variable(tf.random.normal((nb_classes,)), name='bias2')\n",
        "\n",
        "        self.W3 = tf.Variable(tf.random.normal((nb_classes, nb_classes)), name='weight3')\n",
        "        self.b3 = tf.Variable(tf.random.normal((nb_classes,)), name='bias3')\n",
        "\n",
        "        self.W4 = tf.Variable(tf.random.normal((nb_classes, 1)), name='weight4')\n",
        "        self.b4 = tf.Variable(tf.random.normal((1,)), name='bias4')\n",
        "\n",
        "        self.variables = [self.W1,self.b1,self.W2,self.b2,self.W3,self.b3,self.W4,self.b4]\n",
        "\n",
        "    def preprocess_data(self, features, labels):\n",
        "        features = tf.cast(features, tf.float32)\n",
        "        labels = tf.cast(labels, tf.float32)\n",
        "        return features, labels\n",
        "\n",
        "    def deep_nn(self, features):\n",
        "        layer1 = tf.sigmoid(tf.matmul(features, self.W1) + self.b1)\n",
        "        layer2 = tf.sigmoid(tf.matmul(layer1, self.W2) + self.b2)\n",
        "        layer3 = tf.sigmoid(tf.matmul(layer2, self.W3) + self.b3)\n",
        "        hypothesis = tf.sigmoid(tf.matmul(layer3, self.W4) + self.b4)\n",
        "        return hypothesis\n",
        "\n",
        "    def loss_fn(self, hypothesis, features, labels):\n",
        "        cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
        "        return cost\n",
        "\n",
        "    def accuracy_fn(self, hypothesis, labels):\n",
        "        predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "        accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
        "        return accuracy\n",
        "\n",
        "    def grad(self, hypothesis, features, labels):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss_value = self.loss_fn(self.deep_nn(features),features,labels)\n",
        "        return tape.gradient(loss_value,self.variables)\n",
        "\n",
        "    def fit(self, dataset, EPOCHS=20000, verbose=500):\n",
        "        optimizer =  tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "        for step in range(EPOCHS):\n",
        "            for features, labels  in dataset:\n",
        "                features, labels = self.preprocess_data(features, labels)\n",
        "                grads = self.grad(self.deep_nn(features), features, labels)\n",
        "                optimizer.apply_gradients(grads_and_vars=zip(grads, self.variables))\n",
        "                if step % verbose == 0:\n",
        "                    print(\"Iter: {}, Loss: {:.4f}\".format(step, self.loss_fn(self.deep_nn(features),features,labels)))\n",
        "    def test_model(self,x_data, y_data):\n",
        "        x_data, y_data = self.preprocess_data(x_data, y_data)\n",
        "        test_acc = self.accuracy_fn(self.deep_nn(x_data),y_data)\n",
        "        print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
      ],
      "metadata": {
        "id": "Y56xrEDGDcm-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = wide_deep_nn(nb_classes)"
      ],
      "metadata": {
        "id": "tSZqWEvEFlno"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6Q0r5nKFnYh",
        "outputId": "468f0390-e61e-4c0d-ef6a-fed415b559cc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 0, Loss: 1.3109\n",
            "Iter: 500, Loss: 0.7002\n",
            "Iter: 1000, Loss: 0.6930\n",
            "Iter: 1500, Loss: 0.6874\n",
            "Iter: 2000, Loss: 0.6830\n",
            "Iter: 2500, Loss: 0.6792\n",
            "Iter: 3000, Loss: 0.6757\n",
            "Iter: 3500, Loss: 0.6721\n",
            "Iter: 4000, Loss: 0.6682\n",
            "Iter: 4500, Loss: 0.6638\n",
            "Iter: 5000, Loss: 0.6586\n",
            "Iter: 5500, Loss: 0.6525\n",
            "Iter: 6000, Loss: 0.6452\n",
            "Iter: 6500, Loss: 0.6364\n",
            "Iter: 7000, Loss: 0.6256\n",
            "Iter: 7500, Loss: 0.6123\n",
            "Iter: 8000, Loss: 0.5955\n",
            "Iter: 8500, Loss: 0.5740\n",
            "Iter: 9000, Loss: 0.5464\n",
            "Iter: 9500, Loss: 0.5111\n",
            "Iter: 10000, Loss: 0.4675\n",
            "Iter: 10500, Loss: 0.4169\n",
            "Iter: 11000, Loss: 0.3621\n",
            "Iter: 11500, Loss: 0.3066\n",
            "Iter: 12000, Loss: 0.2543\n",
            "Iter: 12500, Loss: 0.2081\n",
            "Iter: 13000, Loss: 0.1694\n",
            "Iter: 13500, Loss: 0.1384\n",
            "Iter: 14000, Loss: 0.1139\n",
            "Iter: 14500, Loss: 0.0949\n",
            "Iter: 15000, Loss: 0.0800\n",
            "Iter: 15500, Loss: 0.0683\n",
            "Iter: 16000, Loss: 0.0590\n",
            "Iter: 16500, Loss: 0.0515\n",
            "Iter: 17000, Loss: 0.0455\n",
            "Iter: 17500, Loss: 0.0405\n",
            "Iter: 18000, Loss: 0.0363\n",
            "Iter: 18500, Loss: 0.0328\n",
            "Iter: 19000, Loss: 0.0298\n",
            "Iter: 19500, Loss: 0.0273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorboard 활용"
      ],
      "metadata": {
        "id": "gd0I2b7wG1So"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_path = \"./logs/xor\"\n",
        "writer = tf.summary.create_file_writer(log_path)"
      ],
      "metadata": {
        "id": "NuQfzxQPG3LV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = tf.Variable(tf.random.normal((2, 10)), name='weight1')\n",
        "b1 = tf.Variable(tf.random.normal((10,)), name='bias1')\n",
        "\n",
        "W2 = tf.Variable(tf.random.normal((10, 10)), name='weight2')\n",
        "b2 = tf.Variable(tf.random.normal((10,)), name='bias2')\n",
        "\n",
        "W3 = tf.Variable(tf.random.normal((10, 10)), name='weight3')\n",
        "b3 = tf.Variable(tf.random.normal((10,)), name='bias3')\n",
        "\n",
        "W4 = tf.Variable(tf.random.normal((10, 1)), name='weight4')\n",
        "b4 = tf.Variable(tf.random.normal((1,)), name='bias4')\n",
        "\n",
        "def neural_net(features, step):\n",
        "    layer1 = tf.sigmoid(tf.matmul(features, W1) + b1)\n",
        "    layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
        "    layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
        "    hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
        "\n",
        "    with writer.as_default():\n",
        "        tf.summary.histogram(\"weights1\", W1, step=step)\n",
        "        tf.summary.histogram(\"biases1\", b1, step=step)\n",
        "        tf.summary.histogram(\"layer1\", layer1, step=step)\n",
        "\n",
        "        tf.summary.histogram(\"weights2\", W2, step=step)\n",
        "        tf.summary.histogram(\"biases2\", b2, step=step)\n",
        "        tf.summary.histogram(\"layer2\", layer2, step=step)\n",
        "\n",
        "        tf.summary.histogram(\"weights3\", W3, step=step)\n",
        "        tf.summary.histogram(\"biases3\", b3, step=step)\n",
        "        tf.summary.histogram(\"layer3\", layer3, step=step)\n",
        "\n",
        "        tf.summary.histogram(\"weights4\", W4, step=step)\n",
        "        tf.summary.histogram(\"biases4\", b4, step=step)\n",
        "        tf.summary.histogram(\"hypothesis\", hypothesis, step=step)\n",
        "\n",
        "    return hypothesis\n",
        "\n",
        "def loss_fn(hypothesis, labels):\n",
        "    cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
        "    with writer.as_default():\n",
        "        tf.summary.scalar('loss', cost, step=step)\n",
        "    return cost\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "\n",
        "def accuracy_fn(hypothesis, labels):\n",
        "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
        "    return accuracy\n",
        "\n",
        "def grad(hypothesis, features, labels, step):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss_fn(neural_net(features, step),labels)\n",
        "    return tape.gradient(loss_value, [W1, W2, W3, W4, b1, b2, b3, b4])"
      ],
      "metadata": {
        "id": "-sGbJ99NG-J7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 3000\n",
        "\n",
        "for step in range(EPOCHS):\n",
        "    for features, labels  in dataset:\n",
        "        features, labels = preprocess_data(features, labels)\n",
        "        grads = grad(neural_net(features, step), features, labels, step)\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W1, W2, W3, W4, b1, b2, b3, b4]))\n",
        "        if step % 50 == 0:\n",
        "            loss_value = loss_fn(neural_net(features, step),labels)\n",
        "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_value))\n",
        "x_data, y_data = preprocess_data(x_data, y_data)\n",
        "test_acc = accuracy_fn(neural_net(x_data, step),y_data)\n",
        "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk1NtzgkHcyM",
        "outputId": "c4e3622a-9b82-40be-b0e6-b52c21c86010"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 0, Loss: 0.7094\n",
            "Iter: 50, Loss: 0.6908\n",
            "Iter: 100, Loss: 0.6854\n",
            "Iter: 150, Loss: 0.6809\n",
            "Iter: 200, Loss: 0.6767\n",
            "Iter: 250, Loss: 0.6723\n",
            "Iter: 300, Loss: 0.6675\n",
            "Iter: 350, Loss: 0.6621\n",
            "Iter: 400, Loss: 0.6559\n",
            "Iter: 450, Loss: 0.6488\n",
            "Iter: 500, Loss: 0.6407\n",
            "Iter: 550, Loss: 0.6315\n",
            "Iter: 600, Loss: 0.6213\n",
            "Iter: 650, Loss: 0.6099\n",
            "Iter: 700, Loss: 0.5974\n",
            "Iter: 750, Loss: 0.5835\n",
            "Iter: 800, Loss: 0.5682\n",
            "Iter: 850, Loss: 0.5514\n",
            "Iter: 900, Loss: 0.5328\n",
            "Iter: 950, Loss: 0.5121\n",
            "Iter: 1000, Loss: 0.4889\n",
            "Iter: 1050, Loss: 0.4628\n",
            "Iter: 1100, Loss: 0.4334\n",
            "Iter: 1150, Loss: 0.4006\n",
            "Iter: 1200, Loss: 0.3649\n",
            "Iter: 1250, Loss: 0.3274\n",
            "Iter: 1300, Loss: 0.2897\n",
            "Iter: 1350, Loss: 0.2536\n",
            "Iter: 1400, Loss: 0.2205\n",
            "Iter: 1450, Loss: 0.1912\n",
            "Iter: 1500, Loss: 0.1661\n",
            "Iter: 1550, Loss: 0.1448\n",
            "Iter: 1600, Loss: 0.1270\n",
            "Iter: 1650, Loss: 0.1121\n",
            "Iter: 1700, Loss: 0.0995\n",
            "Iter: 1750, Loss: 0.0890\n",
            "Iter: 1800, Loss: 0.0801\n",
            "Iter: 1850, Loss: 0.0725\n",
            "Iter: 1900, Loss: 0.0661\n",
            "Iter: 1950, Loss: 0.0605\n",
            "Iter: 2000, Loss: 0.0556\n",
            "Iter: 2050, Loss: 0.0514\n",
            "Iter: 2100, Loss: 0.0477\n",
            "Iter: 2150, Loss: 0.0444\n",
            "Iter: 2200, Loss: 0.0415\n",
            "Iter: 2250, Loss: 0.0389\n",
            "Iter: 2300, Loss: 0.0365\n",
            "Iter: 2350, Loss: 0.0344\n",
            "Iter: 2400, Loss: 0.0325\n",
            "Iter: 2450, Loss: 0.0308\n",
            "Iter: 2500, Loss: 0.0292\n",
            "Iter: 2550, Loss: 0.0278\n",
            "Iter: 2600, Loss: 0.0265\n",
            "Iter: 2650, Loss: 0.0253\n",
            "Iter: 2700, Loss: 0.0241\n",
            "Iter: 2750, Loss: 0.0231\n",
            "Iter: 2800, Loss: 0.0222\n",
            "Iter: 2850, Loss: 0.0213\n",
            "Iter: 2900, Loss: 0.0204\n",
            "Iter: 2950, Loss: 0.0197\n",
            "Testset Accuracy: 1.0000\n"
          ]
        }
      ]
    }
  ]
}